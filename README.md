# Variational Inference for Bayesian Neural Networks

The main focus of this thesis is variational inference for Bayesian neural networks. Bayesian methods naturally allow to quantify the uncertainty, but usually they are difficult to implement efficiently especially for larger datasets or complex models.

The project will start with exploring effective ways of variational learning for neural networks based on recent papers. The next step will be reproducing results from the papers to establish a solid baseline for future experiments.

In the experimental part I will look into optimization strategies for variational training and evaluate them in terms of training stability and performance.

The goal is to identify which strategy and what parameters can provide significant benefits for Bayesian neural networks.

The final outcome will be a set of experiments, a codebase, and a discussion of trade-offs that can help decide when variational methods are worth to consider.
